一、什么是one-hot编码？

One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。

One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。


二、例子
比如：我们要对["中国", "美国", "日本"]进行one-hot编码

1.确定要编码的对象--["中国", "美国", "日本", "美国"]，

2.确定分类变量--中国    美国    日本，共3种类别；

3.以上问题就相当于，有4个样本，每个样本有3个特征，将其转化为二进制向量表示

      中国  美国  日本  美国
中国   1     0      0     0
美国   0     1      0     0
日本   0     0      1     0

三、为什么需要one-hot编码？

one hot编码是将类别变量转换为机器学习算法易于利用的一种形式的过程。

上面的 hello world 相当于多分类的问题（27分类），每个样本只对应于一个类别（即只在对应的特征处值为1，其余地方值为0），
而我们的分类结果，得到的往往是隶属于某个类别的概率，这样在进行损失函数（例如交叉熵损失）或准确率计算时，变得非常方便


四、one-hot编码的缺陷

one-hot编码要求每个类别之间相互独立，如果之间存在某种连续型的关系，或许使用distributed respresentation（分布式）更加合适。

https://www.cnblogs.com/shuaishuaidefeizhu/p/11269257.html






word embedding    最简单的word embedding是把词进行基于词袋（BOW）的One-Hot表示

其实简单说来就是word embedding包含了word2vec，word2vec是word embedding的一种，将词用向量表示。

1.最简单的word embedding是把词进行基于词袋（BOW）的One-Hot表示。这种方法，把词汇表中的词排成一列，对于某个单词 A，如果它出现在上述词汇序列中的位置为 k，
那么它的向量表示就是“第 k 位为1，其他位置都为0 ”的一个向量。但是这种表示方法学习不到单词之间的关系（位置、语义），并且如果文档中有很多词，词向量可能会很长。
对于这两个问题，第一个问题的解决方式是ngram，但是计算量很大，第二个问题可以通过共现矩阵（Cocurrence matrix）解决，但还是面临维度灾难，所以还需降维。

2.现在较常用的方法就是通过word2vec训练词汇，将词汇用向量表示。该模型涉及两种算法：CBOW和Skip-Gram。

cbow是给定上下文来预测中心词，skip-gram是通过中心词预测上下文,两者所用的神经网络都只需要一层hidden layer.

他们的做法是：

cbow:

将一个词所在的上下文中的词作为输入，而那个词本身作为输出，也就是说，看到一个上下文，希望大概能猜出这个词和它的意思。通过在一个大的语料库训练，得到一个从输入层到隐含层的权重模型。如下图所示，第l个词的上下文词是i，j，k，那么i，j，k作为输入，它们所在的词汇表中的位置的值置为1。然后，输出是l，把它所在的词汇表中的位置的值置为1。训练完成后，就得到了每个词到隐含层的每个维度的权重，就是每个词的向量。

skip-gram

将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词。通过在一个大的语料库训练，得到一个从输入层到隐含层的权重模型。如下图所示，第l个词的上下文词是i，j，k，那么i，j，k作为输出，它们所在的词汇表中的位置的值置为1。然后，输入是l，把它所在的词汇表中的位置的值置为1。训练完成后，就得到了每个词到隐含层的每个维度的权重，就是每个词的向量。
