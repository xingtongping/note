监督学习：
有监督：提供数据并提供数据对应的结果
无监督：提供数据但不提供结果
强化：

监督学习主要包括分类和回归

分类：K近邻  、逻辑回归、决策树、朴素贝叶斯法、神经网络、感知机、支撑向量机等
回归：线性回归


分类问题预测数据属于哪一类别 ---离散
回归问题根据数据预测一个数值 ---连续


是否可以转化为是的概率是多少  否的概率多少
分类问题转为回归问题


有监督、无监督、强化学习、深度学习神经网络、集成学习

监督学习步骤：
得到训练集
确定包含所有学习模型的集合
确定模型选择的准则，也就是学习策略
实现最优模型的算法，也就是学习算法
通过学习算法选择最优模型
利用最优模型，对新数据进行预测


三要素：模型  策略  算法


模型评估策略
损失函数
经验风险：训练数据集的平均损失 

训练误差和测试误差
测试误差真正反映了模型对未知数据的预测能力，这种能力一般叫泛化能力


过拟合和欠拟合
过拟合：学得过了
欠拟合：学习的东西少了，欠缺


正则化 防止过拟合，在经验风险加上表示模型复杂度的正则化项，或者叫惩罚项    结构风险最小化
1/N L(y1,f(x1))经验风险
1/N L(y1,f(x1))+ ^J(f)    ^>=0



奥卡姆剃刀
原理：如无必要，勿增实体
正则化符合奥卡姆剃刀的原理：在所有可能的选择模型中，我们应该选择能够很好解释已知数据并且十分简单的模型，如果简单模型已经够用，不应该一味追求更小的训练误差，而把模型变得越来越复杂。


交叉验证：
数据不足，可以重复利用数据
简单交叉验证
数据随机分为两部分，70作为训练集  剩下30%作为测试集



精确率和召回率

TP：正类预测为正类的数目
FN：正类预测为负类的数目  应该推出来的没有推出来
FP：负类预测为正类的数目
TN：负类预测为负类的数目   本来不应该推荐的没把它推荐出来 正确


评价推荐效果的指标

精确率  P= TP/TP+FP        TP+FP推荐出来的那部分  
指的是所有预测为正类的数据中，预测正确的比例

召回率  R = TP/TP+FN    
指的是所有实际为正类的数据中，被正确预测找出的比例




回归问题等价于函数拟合，选择一条函数曲线，使其很好地拟合已知数据，并且能够预测未知数据

回归问题的分类
按照输入变量个数：一元回归和多元回归
按照模型类型：线性回归和非线性回归

回归学习的损失函数--平方损失函数---回归问题用最小二乘法来求解

别的算法：
梯度下降算法      
牛顿法和拟牛顿法       


梯度下降是求解无约束优化问题最简单最经典的方法   梯度：变化最快的方向
梯度方向：函数增长最快的方向




电影相似度 model.productFeatures  推荐电影

拿用户最近的电影，推荐相似的电影
存在问题：看过得电影不一定喜欢
解决：推荐的电影X，和最近几次看的电影相似度加权平均，得出综合分，看值不值得推荐
综合考量最近K次的电影评分

A 5.0         B 4.0       C 1.0       
sim(A,X)   sim(B,X)   sim(C,X) 
D 推荐X电影。X的优先级 =sim(A,X) *5+ sim(B,X)*4 +sim(C,X) *1/3 +lg2 -lg1      对max做奖励  做min做惩罚
